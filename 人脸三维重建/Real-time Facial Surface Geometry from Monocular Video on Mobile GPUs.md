# Abstract
我们提出了一种基于端到端神经网络的模型，用于从 AR 应用程序的单个摄像头输入中推断人脸的近似 3D 网格表示。 468 个顶点的相对密集的网格模型非常适合基于面部的 AR 效果。所提出的模型在移动 GPU 上展示了超实时推理速度（100-1000+ FPS，取决于设备和模型变体）和与同一图像的手动注释方差相当的高预测质量。
# 1.Introduction
通过对齐面部网格模板来预测面部几何形状的问题，也称为面部对齐或面部配准，长期以来一直是计算机视觉的基石。它通常根据定位相对较少（通常为 68 个）的地标或关键点来提出。这些点要么具有自己独特的语义，要么参与有意义的面部轮廓。
另一种方法是估计 3D 可变形模型 (3DMM) [3] 的姿势、比例和参数。一个3DMM，比如BFM2017，2017年版的Basel Face Model[7]，通常是通过主成分分析得到的。生成的网格通常具有更多的点（在 BFM 的情况下大约有 50K），但可能的预测范围受到线性空间的限制，而这又取决于捕获的面部集合的多样性。举一个具体的例子，BFM 似乎无法可靠地表示只闭着一只眼睛的脸。 
我们提出了一个用神经网络估计 3D 网格顶点位置的问题，处理每个顶点作为一个独立的地标。网状拓扑结构由 468 个点组成，这些点排列成固定的四边形。
模型的输入是单个 RGB 相机的帧（或更一般地说，帧流）——不需要深度传感器信息。图 1 显示了模型输出的示例。我们的设置以实时移动 GPU 推理为目标，但我们还设计了该模型的较轻版本，以解决缺乏适当 GPU 支持的移动设备上的 CPU 推理问题。我们将 GPU 目标模型称为“完整”模型，将其与实验中为 CPU 量身定制的“最轻”模型进行对比。
![image.png](https://cdn.nlark.com/yuque/0/2023/png/34409005/1675685754804-26297d45-86fa-4a0f-bd9e-9046325b36c2.png#averageHue=%23d2c8be&clientId=ub424ec71-1d93-4&from=paste&height=478&id=ubadb4445&name=image.png&originHeight=597&originWidth=999&originalType=binary&ratio=1&rotation=0&showTitle=false&size=561423&status=done&style=none&taskId=u16f3afc8-14bc-4831-87b2-c2819e9c417&title=&width=799.2)
# 2. Image processing pipeline
我们组织图像的处理如下：

1. 来自摄像头输入的整个帧由一个非常轻量级的人脸检测器 [2] 处理，该检测器会生成人脸边界矩形和几个地标（例如眼中心、耳道和鼻尖）。地标用于旋转面部矩形以对齐眼睛连接线以矩形的水平轴为中心。
2. 从上一步中获得的矩形从原始图像中裁剪并调整大小，以形成网格预测神经网络的输入（大小范围从完整模型的 256×256 像素到最小模型的 128×128）。该模型生成一个 3D 地标坐标向量，随后将其映射回原始图像坐标系。不同的标量网络输出（人脸标志）产生事件的概率，即合理对齐的人脸确实存在于所提供的作物中。

我们采用的策略是顶点的 x 和 y 坐标对应于图像像素坐标给定的 2D 平面中的点位置。 z 坐标被解释为相对于通过网格质心的参考平面的深度。它们被重新缩放，以便在 x 坐标的跨度和 z 坐标的跨度之间保持固定的纵横比，即缩放到其一半大小的面具有其深度范围（最近到最远）按比例缩小相同的乘数。
当在面部跟踪模式下用于视频输入时，可以从先前的帧预测中获得良好的面部裁剪，并且面部检测器的使用是多余的。在这种情况下，它仅用于第一帧和罕见的重新获取事件（在人脸标志预测的概率低于适当的阈值之后）。
应该注意的是，在这种设置下，第二个网络接收到的输入是人脸合理居中和对齐的。我们认为这可以节省一些模型表示能力，否则这些能力可以用于处理具有大量旋转和平移的案例。特别是，我们可以在获得预测质量的同时减少相关增强的数量
# 3. Dataset, annotation, and training
在我们的训练中，我们依赖于全球来源的数据集，该数据集包含从在不断变化的照明条件下的各种传感器。在训练期间，我们进一步使用标准裁剪和图像处理原语以及一些专门的原语来扩充数据集：建模相机传感器噪声 [8] 并将随机非线性参数变换应用于图像强度直方图（后者有助于模拟边缘光照条件）。
获取 468 个 3D 网格点的基本事实是一项劳动密集型且高度模糊的任务。我们采用以下迭代过程，而不是逐一手动注释点。

1. 使用以下两种监督来源训练初始模型：3DMM 在真实世界照片的面部矩形上的合成渲染（与实体背景相反，以避免过度拟合）。因此，可以从 468 个网格点和 3DMM 顶点子集之间的预定义对应关系中立即获得地面实况顶点坐标。     二维地标对应于参与一组语义轮廓（参见图 3）的网格顶点的一个小子集，这些轮廓在实际的“野外”数据集上进行了注释。地标被预测为专用网络分支末端的单独输出，引入的目的是共享 2D 和 3D 路径之间的中间人脸表示。在训练第一个模型后，我们数据集中多达 30% 的图像具有适合在后续步骤中进行细化的预测。
2. 通过将最新模型应用于图像，过滤掉适合这种细化的那些（即预测误差是可以容忍的），迭代地细化引导的 x 和 y 坐标。快速注释细化由具有可调半径的“刷子”工具实现，可以一次移动整个范围的点。移动量随着从鼠标光标下的枢轴顶点沿着网格边缘的距离呈指数下降。这允许注释者调整大量区域位移局部细化之前的大“笔画”，同时保持网格表面的平滑度。我们注意到 z 坐标保持不变；他们唯一的监督来源是上面概述的合成 3D 渲染。尽管深度预测因此在度量上不准确，但根据我们的经验，生成的网格在视觉上足够合理，例如在面部驱动逼真的 3D 纹理渲染或对齐作为虚拟配件试戴体验的一部分渲染的 3D 对象。
# 4. Model architecture
对于网格预测模型，我们使用自定义但相当简单的残差神经网络架构。我们在网络的早期层使用更积极的子采样，并将大部分计算用于其浅层部分。
因此，神经元接受域相对较早地开始覆盖输入图像的大面积区域。当这样的感受野到达图像边界时，它在输入图像中的相对位置变得隐式可供模型依赖（由于卷积填充）。因此，更深层的神经元可能会区分，例如。嘴巴相关和眼睛相关的特征。
该模型能够完成轻微遮挡或穿过图像边界的面部。这使我们得出一个结论，即由仅在网络的最后几层中转换为坐标的模型构建了高层次和低维的网格表示。
# 5. Filtering for temporal consistency in video
由于我们的模型在单帧级别上运行，因此在帧之间传递的唯一信息是旋转的面部边界矩形（以及是否应该使用面部检测器重新评估）。由于后续视频帧中人脸的像素级图像表示不一致（由于视图的小仿射变换、头部姿势变化、光照变化以及不同类型的相机传感器噪声 [8]），这导致单个地标轨迹中人类可察觉的波动或时间抖动（尽管作为表面的整个网格受此现象的影响较小）。
我们建议通过采用独立应用于每个预测地标坐标的一维时间滤波器来解决这个问题。由于我们提出的管道的主要应用是视觉上吸引人的渲染，我们从人机交互方法中汲取灵感，特别是 1 欧元过滤器 [5]。 1 Euro 和相关滤波器的主要前提是，在降噪和消除相位滞后之间进行权衡，当参数几乎没有变化时，人们更喜欢前者（即稳定），而当参数几乎没有变化时，人们更喜欢后者（即避免滞后）。变化率很高。我们的过滤器保持固定滚动一些用于速度估计的带时间戳样本的窗口，这些样本根据面部大小进行调整以适应视频流中面部比例的变化。使用此过滤器可以在视频上生成吸引人的预测序列，而不会出现可见的抖动。
# 6. Results
我们使用预测和地面真实顶点位置之间的平均绝对距离 (MAD)，通过眼间距离 (IOD) 归一化，定义为眼睛中心之间的距离（估计为眼角连接段的中点，以避免凝视方向依赖）。这种规范化旨在避免考虑面部比例。由于 z 坐标完全是从综合监督中获得的，我们报告了仅 2D 的错误，但 3D 眼间距离用于解释可能的偏航头旋转。
为了量化问题的模糊性并获得我们的指标的基线，我们给了 11 个经过训练的注释器中的每一个注释一组 58 张图像的任务，并计算了它们对同一图像的注释之间的 IOD 归一化平均绝对距离。估计的 IOD MAD 误差为 2.56%。
我们展示了 1.7K 图像的地理多样化评估集的评估结果。速度估计基于 TensorFlow Lite GPU 框架 [1]。
![image.png](https://cdn.nlark.com/yuque/0/2023/png/34409005/1675686578921-f8f481de-3522-4f76-a9ae-23bb33fcbfeb.png#averageHue=%23ebebeb&clientId=ub424ec71-1d93-4&from=paste&height=314&id=u36eb8ce6&name=image.png&originHeight=392&originWidth=997&originalType=binary&ratio=1&rotation=0&showTitle=false&size=48091&status=done&style=none&taskId=udc42dc0e-e9c0-49bd-a97b-e73b80e1229&title=&width=797.6)
本文中描述的技术正在推动手机上主要的 AR 自我表达应用程序和 AR 开发人员 API。图 4 展示了它启用的众多渲染效果的两个示例。
![image.png](https://cdn.nlark.com/yuque/0/2023/png/34409005/1675686617898-62623b9c-2132-4018-bd07-be179c2497c0.png#averageHue=%23a88979&clientId=ub424ec71-1d93-4&from=paste&height=523&id=u531463e8&name=image.png&originHeight=654&originWidth=970&originalType=binary&ratio=1&rotation=0&showTitle=false&size=545337&status=done&style=none&taskId=ue82a024a-36f7-47d8-b51d-01342f5231b&title=&width=776)
